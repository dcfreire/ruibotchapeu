{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6-final"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python39664bit149e8382a9df46e38b7b416944f0faa3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/spmallick/learnopencv/blob/5d9e05bce247e475488a40cfec0de11c006db532/Homography/utils.py\n",
    "\n",
    "def mouse_handler(event, x, y, flags, data) :\n",
    "    \n",
    "    if event == cv2.EVENT_LBUTTONDOWN :\n",
    "        cv2.circle(data['im'], (x,y),3, (0,0,255), 5, 16)\n",
    "        cv2.imshow(\"Image\", data['im']);\n",
    "        if len(data['points']) < 4 :\n",
    "            data['points'].append([x,y])\n",
    "        \n",
    "def get_four_points(im):\n",
    "    \n",
    "    # Set up data to send to mouse handler\n",
    "    data = {}\n",
    "    data['im'] = im.copy()\n",
    "    data['points'] = []\n",
    "    \n",
    "    #Set the callback function for any mouse event\n",
    "    cv2.namedWindow(\"Image\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"Image\",im)\n",
    "    cv2.setMouseCallback(\"Image\", mouse_handler, data)\n",
    "    while cv2.waitKey(0) != ord('q'):\n",
    "        pass\n",
    "    cv2.destroyAllWindows()\n",
    "    # Convert array to np.array\n",
    "    points = np.vstack(data['points']).astype(float)\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/jrosebr1/imutils/blob/master/imutils/convenience.py\n",
    "\"\"\"\n",
    "def grab_contours(cnts):\n",
    "    # if the length the contours tuple returned by cv2.findContours\n",
    "    # is '2' then we are using either OpenCV v2.4, v4-beta, or\n",
    "    # v4-official\n",
    "    if len(cnts) == 2:\n",
    "        cnts = cnts[0]\n",
    "\n",
    "    # if the length of the contours tuple is '3' then we are using\n",
    "    # either OpenCV v3, v4-pre, or v4-alpha\n",
    "    elif len(cnts) == 3:\n",
    "        cnts = cnts[1]\n",
    "\n",
    "    # otherwise OpenCV has changed their cv2.findContours return\n",
    "    # signature yet again and I have no idea WTH is going on\n",
    "    else:\n",
    "        raise Exception((\"Contours tuple must have length 2 or 3, \"\n",
    "            \"otherwise OpenCV changed their cv2.findContours return \"\n",
    "            \"signature yet again. Refer to OpenCV's documentation \"\n",
    "            \"in that case\"))\n",
    "\n",
    "    # return the actual contours array\n",
    "    return cnts"
   ]
  },
  {
   "source": [
    "TESTE DO OTSU PRA CROP -> HOMOGRAPHY\n",
    "\n",
    "https://stackoverflow.com/questions/60780831/python-how-to-cut-out-an-area-with-specific-color-from-image-opencv-numpy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveVideo(frames, size=(640, 480)):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'FMP4')\n",
    "    out = cv2.VideoWriter('output.mp4', fourcc, 0.3, size)\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BLOB TEST\n",
    "\"\"\"\n",
    "import cv2\n",
    "import detector_juba.contours as dj\n",
    "fname = \"./judd.webm\"\n",
    "size = (500, 1000, 3)\n",
    "cap = cv2.VideoCapture(fname)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "homography = dj.get_homography(frame)\n",
    "im_dst = cv2.warpPerspective(frame, homography, size[:2])\n",
    "im = cv2.cvtColor(im_dst, cv2.COLOR_BGR2GRAY)\n",
    "detector = cv2.SimpleBlobDetector()\n",
    "keypoints = detector.detect(im)\n",
    "im_with_keypoints = cv2.drawKeypoints(im, keypoints, np.array([]), (0,0,255),cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'im_with_keypoints' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fed61ef20a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamedWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Result Image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWINDOW_NORMAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Result Image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_with_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'im_with_keypoints' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "cv2.namedWindow('Result Image', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Result Image', im_with_keypoints)\n",
    "while cv2.waitKey(0) != ord('q'):\n",
    "        pass\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "source": [
    "mega clean == blur no output de canny\n",
    "\n",
    "mesa branca == blur em src -> otsu no blur"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detector_juba.contours as dj\n",
    "\n",
    "fname = \"./judd.webm\"\n",
    "size = (500, 1000, 3)\n",
    "cap = cv2.VideoCapture(fname)\n",
    "ret, frame = cap.read()\n",
    "homography = dj.get_homography(frame)\n",
    "im_dst = cv2.warpPerspective(frame, homography, size[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OTSU'S METHOD TEST\n",
    "\"\"\"\n",
    "\n",
    "fname = \"./judd.webm\"\n",
    "cap = cv2.VideoCapture(fname)\n",
    "cap_size = (int(cap.get(3)), int(cap.get(4)))\n",
    "ret, frame = cap.read()\n",
    "\n",
    "src = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "dst = cv2.Canny(src, 50, 100, None, 3)\n",
    "blur = cv2.GaussianBlur(src,(5,5),0)\n",
    "ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "cnts = cv2.findContours(th3, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    #contours, hierarchy = cv2.findContours(th3, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    #cv2.drawContours(image=image_copy, contours=contours, contourIdx=-1, color=(0, 255, 0), thickness=2, lineType=cv2.LINE_AA)\n",
    "image_copy2 = frame.copy()\n",
    "cnts = grab_contours(cnts)\n",
    "cv2.drawContours(image=image_copy2, contours=cnts, contourIdx=-1, color=(0, 0, 255), thickness=4, lineType=cv2.LINE_AA) \n",
    "image_copy = frame.copy()\n",
    "image_copy3 = frame.copy()\n",
    "    # Find the contour with the maximum area.\n",
    "c = max(cnts, key=cv2.contourArea)\n",
    "cv2.drawContours(image=image_copy3, contours=c, contourIdx=-1, color=(0, 0, 255), thickness=4, lineType=cv2.LINE_AA) \n",
    "c = cv2.approxPolyDP(c, 0.02 * cv2.arcLength(c, True), True)\n",
    "cv2.drawContours(image=image_copy, contours=c, contourIdx=-1, color=(0, 0, 255), thickness=20, lineType=cv2.LINE_AA)\n",
    "#lines = cv2.HoughLinesP(th3, 1, np.pi / 180, 150, None, 0, 0)\n",
    "#for line in lines:\n",
    "#\t    x1, y1, x2, y2 = line[0]\n",
    "#\t    cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "#cv2.namedWindow('Result Image', cv2.WINDOW_NORMAL)\n",
    "vid_test = [frame, cv2.cvtColor(src, cv2.COLOR_GRAY2BGR), cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR), cv2.cvtColor(th3, cv2.COLOR_GRAY2BGR), image_copy2, image_copy3, image_copy]\n",
    "cv2.namedWindow('Result Image', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"Result Image\", cv2.cvtColor(th3, cv2.COLOR_GRAY2BGR))\n",
    "while cv2.waitKey(0) != ord('q'):\n",
    "        pass\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveVideo(vid_test, cap_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = cv2.copyMakeBorder(im_dst, 40, 40, 710, 710, cv2.BORDER_CONSTANT,value=[192,196,204])\n",
    "cv2.namedWindow('Result Image', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"Result Image\", aux)\n",
    "while cv2.waitKey(0) != ord('q'):\n",
    "        pass\n",
    "cv2.destroyAllWindows()\n",
    "vid_test.append(aux)\n",
    "vid_test.append(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "eof\n"
     ]
    }
   ],
   "source": [
    "fname = \"./judd.webm\"\n",
    "cap = cv2.VideoCapture(fname)\n",
    "cap_size = (int(cap.get(3)), int(cap.get(4)))\n",
    "#fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "#outv = cv2.VideoWriter('output.avi', cv2.CAP_GSTREAMER, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "vid_test = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"eof\")\n",
    "        break\n",
    "\n",
    "    src = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #dst = cv2.Canny(src, 50, 100, None, 3)\n",
    "    blur = cv2.GaussianBlur(src,(5,5),0)\n",
    "    ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    image_copy = frame.copy()\n",
    "    cnts = cv2.findContours(th3, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    #contours, hierarchy = cv2.findContours(th3, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    #cv2.drawContours(image=image_copy, contours=contours, contourIdx=-1, color=(0, 255, 0), thickness=2, lineType=cv2.LINE_AA)\n",
    "    cnts = grab_contours(cnts) \n",
    "\n",
    "    # Find the contour with the maximum area.\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    c = cv2.approxPolyDP(c, 0.02 * cv2.arcLength(c, True), True) \n",
    "    #cv2.drawContours(image=image_copy, contours=c, contourIdx=-1, color=(0, 0, 255), thickness=20, lineType=cv2.LINE_AA)\n",
    "    vid_test.append(th3)\n",
    "    # Get bounding rectangle\n",
    "    #x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "    # Crop the bounding rectangle out of img\n",
    "    #out = frame[y:y+h, x:x+w, :].copy()\n",
    "    #outv.write(out.astype('uint8'))\n",
    "\n",
    "    #lines = cv2.HoughLinesP(th3, 1, np.pi / 180, 150, None, 0, 0)\n",
    "    #for line in lines:\n",
    "    #        x1, y1, x2, y2 = line[0]\n",
    "    #        cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "\n",
    "    cv2.namedWindow('Result Image', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"Result Image\", th3)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "#outv.release(\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AUTOMATIC TABLE COTH DELETION \n",
    "\"\"\"\n",
    "\n",
    "import detector_juba.contours as dj\n",
    "\n",
    "fname = \"./judd.webm\"\n",
    "size = (500, 1000, 3)\n",
    "cap = cv2.VideoCapture(fname)\n",
    "ret, frame = cap.read()\n",
    "homography = dj.get_homography(frame)\n",
    "im_dst = cv2.warpPerspective(frame, homography, size[:2])\n",
    "\n",
    "hsv = cv2.cvtColor(im_dst, cv2.COLOR_BGR2HSV);\n",
    "h,s,v = cv2.split(hsv);\n",
    "# get uniques\n",
    "unique_colors, counts = np.unique(s, return_counts=True)\n",
    "unique_h, counts_h = np.unique(h, return_counts=True)\n",
    "unique_v, counts_v = np.unique(v, return_counts=True);\n",
    "\n",
    "# sort through and grab the most abundant unique color\n",
    "big_color, big_colorh, big_colorv = None, None, None\n",
    "biggest, biggesth, biggestv = -1, -1, -1\n",
    "for a in range(len(unique_colors)):\n",
    "    if counts[a] > biggest:\n",
    "        biggest = counts[a];\n",
    "        big_color = int(unique_colors[a])\n",
    "for a in range(len(unique_h)):\n",
    "    if counts_h[a] > biggesth:\n",
    "        biggesth = counts_h[a];\n",
    "        big_colorh = int(unique_h[a])\n",
    "for a in range(len(unique_v)):\n",
    "    if counts_v[a] > biggestv:\n",
    "        biggestv = counts_v[a];\n",
    "        big_colorv = int(unique_v[a])\n",
    "# get the color mask\n",
    "marginh, margin, marginv = 5, 80, 120\n",
    "#lowerb = np.array([52, 150, 50])\n",
    "#upperb = np.array([60, 255, 220])\n",
    "lowerb = np.array([big_colorh - marginh, big_color-margin, big_colorv - marginv])\n",
    "upperb = np.array([big_colorh + marginh, big_color+margin, big_colorv + marginv])\n",
    "mask = cv2.inRange(hsv, lowerb, upperb)\n",
    "\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "mask_closing = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel) # Cleans inner points\n",
    "    \n",
    "# Applies the mask to the original frame\n",
    "_,mask_inv = cv2.threshold(mask_closing,5,255,cv2.THRESH_BINARY_INV)\n",
    "masked_img = cv2.bitwise_and(im_dst,im_dst, mask=mask_inv)\n",
    "\n",
    "cv2.namedWindow('Mask', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"Mask\", masked_img)\n",
    "while cv2.waitKey(0) != ord('q'):\n",
    "        pass\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"./judd.webm\"\n",
    "cap = cv2.VideoCapture(fname)\n",
    "\n",
    "shot = []\n",
    "\n",
    "# TODO: Automatic table corner detection\n",
    "ret, frame = cap.read()\n",
    "#cv2.imshow(\"Image\", frame)\n",
    "pts_src = get_four_points(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "eof\n"
     ]
    }
   ],
   "source": [
    "size = (500, 1000, 3)\n",
    "pts_dst = np.array(\n",
    "               [\n",
    "                [0,0],\n",
    "                [size[0] - 1, 0],\n",
    "                [size[0] - 1, size[1] -1],\n",
    "                [0, size[1] - 1 ]\n",
    "                ], dtype=float\n",
    "               )\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"eof\")\n",
    "        break\n",
    "\n",
    "    h, status = cv2.findHomography(pts_src, pts_dst)\n",
    "    im_dst = cv2.warpPerspective(frame, h, size[:2])\n",
    "    cv2.namedWindow('Image', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"Image\", im_dst)\n",
    "    shot.append(im_dst)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"test.jpg\", shot[0])\n",
    "\n",
    "for frame in shot:\n",
    "    # Smooth\n",
    "    frame_blur = cv2.GaussianBlur(frame, (0,0), 2)\n",
    "    frame_hsv = cv2.cvtColor(frame_blur, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Snooker green color range for judd.webm\n",
    "    lowerb = np.array([52, 150, 50])\n",
    "    upperb = np.array([60, 255, 220])\n",
    "    mask = cv2.inRange(frame_hsv, lowerb, upperb)\n",
    "\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    mask_closing = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel) # Cleans inner points\n",
    "    \n",
    "    # Applies the mask to the original frame\n",
    "    _,mask_inv = cv2.threshold(mask_closing,5,255,cv2.THRESH_BINARY_INV)\n",
    "    masked_img = cv2.bitwise_and(frame,frame, mask=mask_inv)\n",
    "\n",
    "    ctrs, hierarchy = cv2.findContours(mask_inv, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    ctrs = [ctr for ctr in ctrs\n",
    "                        if 300 < cv2.contourArea(ctr) < 1000\n",
    "                        #and cv2.minAreaRect(ctr)[1][0]*0.28 < cv2.minAreaRect(ctr)[1][1]\n",
    "                        #and cv2.minAreaRect(ctr)[1][1]*0.28 < cv2.minAreaRect(ctr)[1][0]\n",
    "            ]\n",
    "    frame = frame.copy()\n",
    "    #draw_rectangles(ctrs, frame)\n",
    "    cv2.drawContours(frame, ctrs, -1, (255, 0, 0), 3)\n",
    "    cv2.namedWindow('FRAME', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"FRAME\", frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = cv2.selectROI(\"tracker\", shot[0])\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.5.3) /build/opencv/src/opencv-4.5.3/modules/video/src/tracking/detail/tracking_feature.cpp:128: error: (-215:Assertion failed) !patchSize.empty() in function 'generateRandomFeature'\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-3af323558471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackerMIL_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.3) /build/opencv/src/opencv-4.5.3/modules/video/src/tracking/detail/tracking_feature.cpp:128: error: (-215:Assertion failed) !patchSize.empty() in function 'generateRandomFeature'\n"
     ]
    }
   ],
   "source": [
    "tracker = cv2.TrackerMIL_create()\n",
    "tracker.init(shot[0], roi)\n",
    "for frame in shot:\n",
    "    frame = frame.copy()\n",
    "    _, roi = tracker.update(frame)\n",
    "    cv2.rectangle(frame, roi, (255, 0, 0), 2, 1)\n",
    "    cv2.imshow(\"tracker\", frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = cv2.TrackerCSRT(shot[0], roi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'MultiTracker'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-642c9112b640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrackers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'MultiTracker'"
     ]
    }
   ],
   "source": [
    "trackers = cv2.legacy.MultiTracker_create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}